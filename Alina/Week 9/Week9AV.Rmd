---
title: "Week 9: Web APIs"
author: "Alina Vikhnevich"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warnings=FALSE)
```

# **Introduction**

For this assignment, I explored how to connect to a RESTful API and extract structured data from the web. I used the *New York Times Most Popular API*, which provides access to a feed of the most-emailed articles over the past week. The overall objective was to pull this data in JSON format, parse it, clean it, and finally transform it into a tidy R `data.frame`, which I then saved as an Excel file for future use.

This exercise gave me hands-on experience with API calls using `httr2`, working with JSON responses, and cleaning nested data structures skills that are crucial for web-based data acquisition and preparation in real-world data science workflows.

## **Load Required Libraries**

```{r load-libraries, warning = FALSE}
# Load required libraries
library(httr2) # For making HTTP requests
library(jsonlite) # For parsing JSON
library(tidyverse) # For data manipulation and cleaning
library(writexl) # To save the result
```

I started by loading all the libraries needed to complete the task. `httr2` is used to perform the HTTP request to the API. `jsonlite` helps parse the JSON response into a format that R can work with. `tidyverse` is essential for data wrangling, and `writexl` lets me export the cleaned dataset to Excel for easier review and sharing.

## **1. Store API Key and Send the Request**

```{r api-key}
# Store your API key
api_key <- Sys.getenv("NYT_API_KEY")

# Create and send the request
resp <- request("https://api.nytimes.com/svc/mostpopular/v2/emailed/7.json") %>%
  req_url_query("api-key" = api_key) %>%
  req_perform()
```

I registered for an API key from the NYT Developer Portal. To securely manage access credentials, I stored my New York Times API key in the `.Renviron` file and retrieved it in the script using `Sys.getenv("NYT_API_KEY")`. It keeps the key hidden from the rendered document. Then, I constructed the API request using `httr2`, added the key as a query parameter, and performed the request. This returned a live HTTP response containing JSON data of the top-emailed articles.

## 2. Parse JSON Content

```{r parse}
# Parse the JSON content
resp_text <- resp_body_string(resp)
data_parsed <- fromJSON(resp_text, flatten = TRUE)
```

The raw JSON content from the API response was first converted into a character string, and then parsed using `fromJSON()`. I used `flatten = TRUE` so that any nested data structures were simplified into a flat data frame format. This helps avoid complex list-columns later in the analysis.

## 3. Extract the Articles Section

```{r extract-data}
# Extract just the 'results' section
articles_df <- data_parsed$results

# Take a quick look
glimpse(articles_df)
```

The API response includes metadata along with the actual content we’re interested in. I isolated the `results` list, which contains the articles themselves, and assigned it to a new data frame.

## 4. Clean and Tidy the Dataset

```{r clean}
# Clean and structure the data
clean_articles <- articles_df %>%
  select(
    title,
    byline,
    section,
    published_date,
    source,
    abstract,
    url
  ) %>%
  mutate(across(everything(), ~ ifelse(. == "" | is.na(.), "None", .)))

# Preview the cleaned dataset
head(clean_articles)
```

The raw dataset contained many columns, some of which were either nested or not useful for my purposes. I kept only the key columns needed for a readable summary: title, author, section, publication date, source, summary, and URL. To handle missing `byline` entries, I replaced empty or `NA` values with `"No Author"`, which keeps the dataset more readable.

## 5. Save the Results to Excel

```{r save-xlsx}
# Save cleaned version to Excel file
write_xlsx(clean_articles, "nyt_popular_articles.xlsx")
```

Once the data was cleaned, I exported it to an Excel file. I chose .xlsx instead of .csv to avoid encoding issues with special characters (e.g., apostrophes and quotation marks) that showed up when exporting to CSV. This method ensured all article titles and abstracts remained readable and intact.

## **Conclusion**

Working through this assignment gave me a clearer understanding of how to source data programmatically using a web API. The process began with creating and sending an authenticated request using my API key via the `httr2` package. I found `httr2` to be very intuitive once I got the structure of the request right.

Parsing the JSON response into an R-readable format using `jsonlite::fromJSON()` with `flatten = TRUE` was a critical step that helped reduce nested complexity early on. After extracting just the `results` portion of the response, I focused on selecting and cleaning the relevant columns. This included handling missing author names in the `byline` column and preparing the dataset for analysis or export.

One minor challenge I ran into was character encoding when attempting to save the dataset as a `.csv` - certain special characters in article titles and abstracts didn’t render properly. Switching to an `.xlsx` format using `writexl::write_xlsx()` solved the issue and preserved the formatting, ensuring the data remained clean and readable.
