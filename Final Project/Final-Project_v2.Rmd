---
title: "DATA 607 Final Project: Pop Culture & Public Search Behavior"
author: "Group Members: Alina Vikhnevich, Olivia Azevedo, Alyssa Gurkas"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: cerulean
    toc: true
    toc_float: true
    toc_collapsed: true
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## Packages

```{r setup, message=FALSE, warning=FALSE}
library(bigrquery)
library(DBI)
library(tidyverse)
library(tidytext)
library(lubridate)
library(wordcloud)
library(topicmodels)
library(countrycode)
```

This project uses several R packages to support data acquisition,
cleaning, analysis, and visualization. For example, `bigrquery` and
`DBI` are used to query data from Google BigQuery, `tidytext` enables
text mining and topic modeling, and `ggplot2` is used for data
visualization.

# Introduction

In this project, we explore how search behavior in the United States
reflects interest in major pop culture events. Using unsupervised topic
modeling, we analyze Google’s top search terms to identify recurring
themes and trends. By linking search activity with globally documented
events, we aim to uncover how societal attention manifests in digital
behavior.

## Data Sources

-   **Google BigQuery – Top 25 Search Terms**\
    Google’s public dataset on the top 25 search queries per day,
    available via BigQuery.
-   **GDELT 2.0 – Our Global World in Realtime**\
    A large-scale global event database tracking news events from around
    the world in real time, including media coverage of pop culture,
    conflict, disasters, and more.

## Data Acquisition

### 1. Google BigQuery – Top 25 Search Terms

We use the `bigrquery` package to access Google’s public dataset of the
most popular daily search terms. Make sure you have a Google Cloud
project set up with billing enabled and a BigQuery API key.

```{r bigquery-auth, eval=FALSE}
# Authenticate with Google Cloud (interactive)
bq_auth(path = "data607finalproject-6c666e5b6214.json")
```

```{r google-bigquery-api}
project_id <- "data607finalproject"

# gbigquery is in GMT so if we do sub date -1 that means that it is taking the 
# data from today, need to do -2
# need to specify that the date is in America/NY time: CURRENT_DATE(‘America/New_York’)

# dma = designated market area
# need to pick two dmas if want to use score **
query <- as.character(
  "SELECT
  'top_terms' as term_type,
  term,
  score,
  refresh_date,
  dma_id,
  dma_name,
  ARRAY_AGG(STRUCT(rank,week) ORDER BY week DESC LIMIT 1) x
FROM
    `bigquery-public-data.google_trends.top_terms`
WHERE refresh_date >= date_add(current_date('America/New_York'),interval - 2 week)
AND regexp_contains(dma_name, 'New York|Los Angeles')
GROUP BY ALL

union  all

SELECT
  'rising_terms' as term_type,
  term as term_rising,
  score as score_rising,
  refresh_date,
  dma_id,
  dma_name,
  ARRAY_AGG(STRUCT(rank,week) ORDER BY week DESC LIMIT 1) x
FROM
    `bigquery-public-data.google_trends.top_rising_terms`
WHERE refresh_date >= date_add(current_date('America/New_York'),interval - 2 week)
AND regexp_contains(dma_name, 'New York|Los Angeles')
GROUP BY ALL
ORDER BY
    (select week from unnest(x)) desc,
    (SELECT rank FROM UNNEST(x)),
    refresh_date desc
"
)

# Run dry run query to estimate billing
job <- bq_perform_query_dry_run(
  billing = project_id,
  query = query,
  use_legacy_sql = FALSE
)

google_search_data <- bq_project_query(project_id, query)
google_search_df <- bq_table_download(google_search_data)
unnest_google_search_df <- google_search_df  |> 
  unnest(x)

head(unnest_google_search_df)
```

### 2. GDELT 2.0: Our Global World in Realtime

**Getting the latest file name:**

```{r gdelt-latest-link}
# Get the name of the most recent GDELT CSV file
latest_file <- read_lines("http://data.gdeltproject.org/gdeltv2/lastupdate.txt")
latest_file
```

**Download and Unzip:**

```{r gdelt-download}
# Constructing the full URL to download the ZIP file
export_url_line <- latest_file[1]

# Extract the URL from the line using strsplit
export_url <- strsplit(export_url_line, " ")[[1]][3]
export_url

# Download the export ZIP file to your working directory
download.file(export_url, destfile = "Data/GDELT/gdelt_latest.zip", mode = "wb")

# Unzip the file
unzip("Data/GDELT/gdelt_latest.zip", exdir='Data/GDELT')
```

**Read CSV into R:**

```{r gdelt-extract}
# Extract filename only from URL
csv_file <- paste0('Data/GDELT/', gsub(".zip", "", basename(export_url)))


# Read the CSV file with correct case-sensitive name
gdelt_df <- read_csv(csv_file, col_names = FALSE)

# Preview data
head(gdelt_df)
```

**GDELT 2.0 – Structured Load**

GDELT 2.0 is a massive global event database that captures media
coverage of events in real time from across the world. The export files
are updated every 15 minutes and are tab-delimited without column
headers, so we must assign names manually using the GDELT schema.

In this chunk, we load the most recent export file and apply column
names for easier exploration:

```{r gdelt-read}
# Column names based on GDELT 2.0 schema (first 10 for demo, full list has 58+)
gdelt_cols <- c(
  "GLOBALEVENTID", "Day", "MonthYear", "Year", "FractionDate", 
  "Actor1Code", "Actor1Name", "Actor1CountryCode", "Actor1KnownGroupCode", 
  "Actor1EthnicCode", "Actor1Religion1Code", "Actor1Religion2Code", 
  "Actor1Type1Code", "Actor1Type2Code", "Actor1Type3Code",
  "Actor2Code", "Actor2Name", "Actor2CountryCode", "Actor2KnownGroupCode", 
  "Actor2EthnicCode", "Actor2Religion1Code", "Actor2Religion2Code", 
  "Actor2Type1Code", "Actor2Type2Code", "Actor2Type3Code",
  "IsRootEvent", "EventCode", "EventBaseCode", "EventRootCode", 
  "QuadClass", "GoldsteinScale", "NumMentions", "NumSources", 
  "NumArticles", "AvgTone", "Actor1Geo_Type", "Actor1Geo_FullName", 
  "Actor1Geo_CountryCode", "Actor1Geo_ADM1Code", "Actor1Geo_ADM2Code",
  "Actor1Geo_Lat", "Actor1Geo_Long", "Actor1Geo_FeatureID", "Actor2Geo_Type", 
  "Actor2Geo_FullName", "Actor2Geo_CountryCode", "Actor2Geo_ADM1Code",
  "Actor2Geo_ADM2Code", "Actor2Geo_Lat", "Actor2Geo_Long", "Actor2Geo_FeatureID", 
  "ActionGeo_Type", "ActionGeo_FullName", "ActionGeo_CountryCode", 
  "ActionGeo_ADM1Code", "Action2Geo_ADM2Code", "ActionGeo_Lat", "ActionGeo_Long", 
  "ActionGeo_FeatureID", "DATEADDED", "SOURCEURL"
)

# Read the file with tab delimiter and custom column names
gdelt_df <- read_delim(csv_file, delim = "\t", col_names = gdelt_cols, show_col_types = FALSE)

# Preview first rows
head(gdelt_df)
```

**Load GDELT Reference Table Data**

Define reference table data file sources
```{r}
cameo_country_code_txt <- "https://www.gdeltproject.org/data/lookups/CAMEO.country.txt"
fips_country_code_txt <- 'https://www.gdeltproject.org/data/lookups/FIPS.country.txt'
cameo_code_type_txt <- 'https://www.gdeltproject.org/data/lookups/CAMEO.type.txt'
cameo_known_code_groups_txt <- 'https://www.gdeltproject.org/data/lookups/CAMEO.knowngroup.txt'
cameo_ethnic_code_txt <- 'https://www.gdeltproject.org/data/lookups/CAMEO.ethnic.txt'
cameo_religion_code_txt <- 'https://www.gdeltproject.org/data/lookups/CAMEO.religion.txt'
cameo_event_code_txt <- 'https://www.gdeltproject.org/data/lookups/CAMEO.eventcodes.txt'
cameo_gold_stein_scale_txt <- 'https://www.gdeltproject.org/data/lookups/CAMEO.goldsteinscale.txt'
adm1_codes_txt <- 'http://efele.net/maps/fips-10/data/fips-414.txt'
adm2_codes_txt <- 'https://download.geonames.org/export/dump/admin2Codes.txt'
```

Actor Data Reference Tables
```{r actor-ref}
# Actor Country Code column reference values
cameo_country_ref <- read.csv(
  cameo_country_code_txt, sep="\t", header=T,
  col.names = c('Country', 'Country_Desc'))

# Actor Known Code Group column reference values
cameo_known_code_groups_ref <- read.csv(
  cameo_known_code_groups_txt, sep="\t", header=T,
  col.names = c('KnownGroup', 'KnownGroup_Desc'))

# Actor Ethnic Code column reference values
cameo_ethnic_ref <- read.csv(cameo_ethnic_code_txt, sep="\t", header=T,
                            col.names = c('Ethnic', 'Ethnic_Desc'))

# Actor Religion Code (1 and 2) column reference values
cameo_religion_ref <- read.csv(cameo_religion_code_txt, sep="\t", header=T,
                              col.names = c('Religion', 'Religion_Desc'))

# Actor Type Code (1, 2, and 3) column reference values
cameo_type_ref <- read.csv(cameo_code_type_txt, sep="\t", header=T,
                                col.names = c('TypeCode', 'Type'))
```

Event Data Reference Tables
```{r}
# Event Code (including base and root event codes) column reference values
cameo_event_ref <- read.csv(cameo_event_code_txt, sep="\t", header=T ,
                           colClasses = "character",
                                col.names = c('EventCode', 'Event'))

# Event Gold Stein Scale column reference values
cameo_gold_stein_ref <- read.csv(cameo_gold_stein_scale_txt, sep="\t", header=T,
                                col.names = c('EventCode', 'GoldSteinScale'))

# Event quad class (primary classification) column reference values
quadclass_ref <- tribble(
  ~"QuadClass", ~"Quad",
  1, 'Verbal Cooperation',
  2, 'Material Cooperation',
  3, 'Verbal Conflict',
  4,' Material Conflict'
)
```

Event Geography Data Reference Tables
```{r event-ref}
# Event geographic resolution column reference values
geo_type_ref <- tribble(
  ~"Geo_Type", ~"Geo_Type_Desc",
  1, 'COUNTRY',
  2, 'USSTATE',
  3, 'USCITY',
  4,' WORLDCITY',
  5, 'WORLDSTATE'
)

# FIPS country column reference values
fips_country_ref <- read.csv(fips_country_code_txt, sep="\t", header=F,
                            col.names = c('GEO_Country', 'GEO_Country_Desc'))

# GEO ADM1 code column reference values
# create U.S. adm1 codes using 'US' followed by the state abbreviation
us_fips_adm1_ref <- tibble(
  'Country' = 'US',
  'division' = 'state',
  fips_adm1 = state.name,
  state_abb = state.abb
  ) |>
  mutate(fips_adm1_code = paste0(Country, state_abb)) |>
  select(fips_adm1_code, division, fips_adm1)

# read in global adm1 codes and join to custom U.S. codes
fips_adm1_ref <- read.csv(
  adm1_codes_txt, sep="_", header=F, na.strings = "",
  col.names = c('fips_adm1_code',
                'first fips version',
                'last fips version',
                'division',
                'designation_2',
                'designation_3',
                'designation4',
                'fips_adm1',
                'convential name',
                'former name')) |>
  select('fips_adm1_code', 'division', 'fips_adm1') |>
  rbind(us_fips_adm1_ref)
```


## Tidy & Clean Data

### 1. Google BigQuery – Top 25 Search Terms

**Google Search Data Cleaning**

  - Transforms `dma_id` into a string dtype
  - Fills Null top_score values with zero
  - Unnests `rising_terms_gains` column to create a long-format dataframe where
    each rising term and associated percent gain is a separate row.
  - Splits `rising_terms_gains` into `rising_term` and `rising_percent_gain` to
    make each value its own column

```{r google-data-cleaning}
long_google_search_df <-  unnest_google_search_df |>
  mutate(
    score = replace_na(score, 0)
    ) |> 
  mutate(search_id = row_number())

head(long_google_search_df)
```

**Google Search Data Tidying**

Subset refresh date into its own table for reference. This is the same for all
rows so only one value is needed to store,
```{r google-refresh-date-ref-table}
refresh_date_ref <- long_google_search_df |>
  select(refresh_date) |>
  distinct()

head(refresh_date_ref)
```

Create Designated Market Area (DMA) reference table.
```{r google-dma-ref-table}
dma_ref_df <- long_google_search_df |>
  select(dma_id, dma_name) |>
  distinct()

head(dma_ref_df)
```

Create final Google top term dateframe
```{r google-top-term-table}
top_term_df <- long_google_search_df |>
  filter(term_type == "top_terms")

head(top_term_df)
```

Create final Google rising term dataframe
```{r google-rising-term-table}
rising_term_df <- long_google_search_df |>
  filter(term_type == "rising_terms")

head(rising_term_df)
```


--
### 2. GDELT 2.0: Our Global World in Realtime

**GDELT Data Cleaning**

  - Transforms `Day` into date data type
  - Transforms `MonthYear` into datetime string format
  - Transforms `GLOBALEVENTID` into strings
  - Transforms all categorical columns into factor data type

```{r gdelt-data-cleaning}
gdelt_df_cleaned <- gdelt_df |>
  mutate_at(vars(contains('Code')), ~as.factor(.)) |>
  mutate_at(vars(contains('Geo_Type')), ~as.factor(.)) |>
  mutate(
    GLOBALEVENTID = as.character(GLOBALEVENTID),
    Day = as.Date(as.character(Day), '%Y%m%d'),
    MonthYear = format(Day, '%Y-%m'),
    QuadClass = as.factor(QuadClass),
  ) |> 
  filter(Day >= today() - days(14) & Day <= today())
```

**GDELT Data Tidying**

No tidying was needed on this dataframe.


---
## Matching the Google Big Query Data with the GDELT Data
```{r join-tbls}
gsearch_gdelt <- 
  left_join(long_google_search_df, 
            gdelt_df_cleaned,
            by=c('refresh_date'='Day'))
```

## EDA
Reference here: https://rpubs.com/jmhardaw/eci588-final-project

```{r tokenize-words}
# getting stop words for english and spanish 
multi_lang_stopwords <- bind_rows(
  get_stopwords(language = "en"),
  get_stopwords(language = "es")
)

tokenized_gsearch_gdelt <- gsearch_gdelt |> 
  unnest_tokens(output = word, input = term) |> 
  anti_join(multi_lang_stopwords, by = "word") |> 
  relocate(search_id, word)
```

```{r count-most-freq-words}
most_freq_tokens <- tokenized_gsearch_gdelt |> 
  count(word, sort = TRUE) |> 
  slice_max(order_by = n, n = 25)
```

```{r}
tokenized_gsearch_gdelt |> 
  group_by(refresh_date) |> 
  count(word, sort = TRUE) |> 
  top_n(5) |> 
  ungroup() |> 
  mutate(word = reorder_within(word, n, refresh_date),
         date = factor(refresh_date, levels=sort(unique(tokenized_gsearch_gdelt$refresh_date),decreasing=TRUE),ordered = TRUE))|>
  ggplot(aes(x = word, y = n, fill = word)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ date, scales = "free_y") +
  coord_flip() +
  scale_x_reordered() +
  scale_y_continuous(expand = c(0,0)) +
  theme_minimal()+
  labs(y = "Count",
       x = "Unique words",
       title = "Most frequently googled words in past two weeks",
       subtitle = "Stop words removed from the list")
```


```{r word-stem}
wrd_stem <- tokenized_gsearch_gdelt %>% 
  mutate(word = SnowballC::wordStem(word))
```

```{r dtm-top-term}
# creating document term matrix 
dtm_top_term <- wrd_stem |> 
  count(refresh_date, word) |> 
  cast_dtm(document = refresh_date, term = word, value = n)
```

```{r lda-top-term}
lda_top_term_model <- LDA(dtm_top_term, k = 6, control = list(seed = 1234))
topics_top_term <- tidy(lda_top_term_model, matrix = "beta")

# Top terms per topic
top_terms <- topics_top_term |>   
  group_by(topic) |> 
  top_n(10, beta) |> 
  ungroup()
```

[OA - The below code was added by Alina. I moved it to the EDA section. I think we may need to look at score/rank rather than count since I think week-dma will all have the same or close to the same top terms.]

### 1. Google BigQuery – Top 25 Search Terms

**Count frequency of top search terms**

This shows how often each top-ranked term appeared across all DMAs and
dates in your sample.

```{r top-term-frequency}
# Count appearances of each top term
top_term_counts <- top_term_df |>
  count(top_term, sort = TRUE)

head(top_term_counts)
```

**Count frequency of rising terms**

This does the same for rising terms, now that we’ve unnested them.

```{r rising-term-frequency}
rising_term_counts <- rising_term_df |>
  count(rising_term, sort = TRUE)

head(rising_term_counts)
```

**Preview which terms are most common**

These previews help you quickly spot which types of trends (e.g.,
sports, celebrity) show up the most.

```{r preview-trending-terms}
# View most frequent top and rising terms side-by-side
top_term_counts |> head(10)
rising_term_counts |> head(10)
```





