---
title: "Project 3: Project Proposal"
author: "Group Members: Alina Vikhnevich, Olivia Azevedo, Alyssa Gurkas, Musrat Jahan"
date: "`r Sys.Date()`"
format:
  revealjs:
    theme: cerulean
    toc: true
    toc-depth: 2
    slide-level: 2
    transition: fade
execute:
  echo: true
---

```{r load-libraries}
library(tidyverse)
library(stringi)
library(DiagrammeR)
library(dplyr)
library(tidyr)
library(stringr)
library(kableExtra)
library(jsonlite)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
This project explores what are the most valuable data science skills. To answer 
this, the following methodology was followed:

- **Data Collection** - Import data from various sources such as Bureau of Labor 
Statistics, Projections Central, and O*Net.
- **Data Normalization** - Clean and normalize the data using various processing 
                           techniques.
- **Export to Database** - Store the processed data.
- **Data Analysis** - Conduct analysis on the structured data.
- **Summary of Findings** - Summarize key insights.

### Research Questions

1.  Which skills are considered the most important in the data science
    field?
2.  What is relationship between projected employment and the importance
    of job skills?
3.  How does the presence of 'hot technologies' relate to skill importance
    in data science roles?
4.  What types of technical skills (based on commodity categories) are
    most commonly marked as in-demand or hot?
5.  What is the distribution of skill importance across different skill
    categories (e.g., cognitive, interpersonal)?

## Data Sources

- [Industry Profile for Data Scientists](https://www.bls.gov/oes/current/oes150000.htm)
- [Projections Central](https://projectionscentral.org/directdownloads)
- [O\*Net Database](https://www.onetcenter.org/database.html#all-files)

## Logical Model

```{r logical-model}   
grViz("
digraph Logical_Model {

  graph [layout = dot, rankdir = TB]

  # Define node styles
  node [shape = rectangle, style = filled, fillcolor = lightblue]

  # Data Sources
  DataSources [label = 'Raw Data Sources', shape = parallelogram, fillcolor = lightgray]
  ONET [label = 'O*NET Database \\n(Skills & Job Data)']
  BLS [label = 'BLS Employment Statistics \\n(Job Salaries)']
  Projections [label = 'Projections \\n(Long-term projections for employment)']
  

  # Processing
  Processing [label = 'Data Processing & Normalization', shape = ellipse, fillcolor = lightyellow]
  Cleaning [label = 'Data Cleaning & \\n Transformation']
  Normalization [label = 'Normalization of \\n Job & Skill Data']

  # Structured Data Tables (Fixed Black Block)
  StructuredData [label = 'Final Structured Data', shape = parallelogram, fillcolor = lightgray, style = filled]
  Core [label = 'Core Tables']
  Link [label = 'Link Tables']
  Ref [label = 'Reference Tables']
  

  # Relationships
  DataSources -> {ONET BLS Projections}
  {ONET BLS Projections} -> Processing
  Processing -> {Cleaning Normalization}
  {Cleaning Normalization} -> StructuredData
  StructuredData -> {Core Link Ref}
}
")
```


## Entity Relationship Diagram
```{r erd}
grViz("
digraph ER_Diagram {
  
  graph [layout = dot, rankdir = LR]
  
  # Define node styles
  node [shape = rectangle, 
        style = filled, 
        fillcolor = lightblue]

  # Core Tables
  TechSkills [label = 'tech_skills\\n(onet_soc,
                                      \\ncommodity_code,
                                      \\nhot_technology,
                                      \\nin_demand)']
                                      
  EpSkills [label = 'ep_skills_df\\n(soc,
                                     \\nep_skills_category_id,
                                     \\nep_skills_score,
                                     \\nonet_element_id,
                                     \\nonet_rating_value)']
                                     
  SocIndustryProject [label = 'soc_industry_project_df\\n(industry_code,
                                                          \\nsoc,
                                                          \\nemployment_2023,
                                                          \\nprct_soc_2023,
                                                          \\nemployment_2033,
                                                          \\nemployment_change_num)']
  SocOES [label = 'soc_oes_df\\n(soc,
                                \\nemployment,
                                \\nwage_stat_duration,
                                \\nwage_statistic,
                                \\nwage_value)']
  IndustryOES [label = 'industry_oes_df\\n(industry_code,
                                           \\nemployment,
                                           \\nwage_value)']

  # Link Tables
  SocOnetLink [label = 'soc_onet_soc_lnk\\n(soc,
                                            \\nonet_soc)']
  SocIndustryLink [label = 'soc_industry_lnk\\n(soc,
                                                \\nindustry_code)']

  # Reference Tables
  CommodityRef [label = 'commodity_ref\\n(commodity_code,
                                          \\ncommodity_title)', 
                                          fillcolor = lightgray]
  SkillsElementRef [label = 'skills_element_ref\\n(onet_element_id,
                                                    \\nonet_element_name)', 
                                                    fillcolor = lightgray]
  SocRef [label = 'soc_ref\\n(soc,
                              \\nsoc_title,
                              \\nsoc_type)', 
                              fillcolor = lightgray]
  SkillsCategoryRef [label = 'skills_category_ref\\n(ep_skills_category_id,
                                                      \\nep_skills_category)', 
                                                      fillcolor = lightgray]
  IndustryRef [label = 'industry_ref\\n(industry_code,
                                        \\nindustry_title,
                                        \\nindustry_type)', 
                                        fillcolor = lightgray]

  # Relationships (Many-to-Many)
  TechSkills -> SocOnetLink [label = 'onet_soc']
  EpSkills -> SocOnetLink [label = 'soc']
  EpSkills -> SkillsElementRef [label = 'onet_element_id']
  EpSkills -> SkillsCategoryRef [label = 'ep_skills_category_id']
  
  SocIndustryProject -> SocIndustryLink [label = 'soc']
  SocIndustryProject -> IndustryRef [label = 'industry_code']
  SocOES -> SocRef [label = 'soc']
  IndustryOES -> IndustryRef [label = 'industry_code']

  SocOnetLink -> SocRef [label = 'soc']
  SocIndustryLink -> IndustryRef [label = 'industry_code']

  # Additional Joins
  TechSkills -> CommodityRef [label = 'commodity_code']
}
")
```


## Data Normalization* 
To reduce redundancy and improve data integrity, the data in this project was 
normalized. This helps to ensure that data is stored efficiently, avoiding 
duplication and inconsistencies, and to have a better-managed database. To 
normalize the datasets, five core tables were developed, and five reference 
tables were developed. 

## Unique Identifiers
This project pulls data from three sources and matches on two IDs: the Standard 
Occupational Classification (SOC) and the North American Industry Classification 
System (NAICS) codes. The SOC codes work as a tree structure. The more digits 
added to the 6-digit SOC code (or 6 digit with 2 decimals O*NET soc code), the 
more in depth the occupation description becomes.  

## Load Data

```{r load-data}
# insert connection to relational database
```

## Tidy Data
To tidy and normalize the data the team performed the following: 
1. Renamed columns to allow for more intuitive names as well as ensure columns 
representing the same data values are referenced the same across all data 
frames. 
2. Developed reference tables to store distinct categorical values 
(e.g., skill categories) and remove partial dependencies. 
3. Removed redundant columns (such as columns that are represented in reference 
tables) from the core data tables, retaining only relevant fields for analysis. 
4. Transformed the OES and Industry and Occupation Projection datasets from wide 
to  long  to make the data tidy and allow for better usability. By transforming 
these datasets each variable is its own column, and each observation is its own 
row (i.e., reducing multiple hourly and annual wage statistics columns into 
three columns for duration, type, and value); thus, making the data tidy.


## Data Analysis

```{r analyze-data}
# insert code analyzing data 

```

## Findings

```{r display-findings}
# insert visualizations
```

## Conclusion

Summarize the key insights and implications of the study.

---