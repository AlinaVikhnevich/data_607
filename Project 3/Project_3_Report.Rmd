---
title: "Project 3: Project Proposal"
author: "Group Members: Alina Vikhnevich, Olivia Azevedo, Alyssa Gurkas, Musrat Jahan"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: cerulean
    toc: true
    toc_float: true
    toc_collapsed: true
editor_options: 
  markdown: 
    wrap: 72
---

```{r chunk-settings, include=FALSE}
# to be updated 
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, include=FALSE}
library(tidyverse)
library(stringi)
# library(DiagrammeR)
library(dplyr)
library(tidyr)
library(stringr)
library(kableExtra)
library(jsonlite)

knit_table <- function(df, caption='', position = 'left') {
  kbl(df, format = "html", escape = FALSE, caption = caption) |>
    kable_styling(
      full_width = F,
      position = position,
      bootstrap_options = c("striped")) |>
    row_spec(0, bold = T,
             color = "white",
             background = "#327fac",
             extra_css = "border: 2px solid #327fac;") |>
    row_spec(dim(df)[1], extra_css = "border-bottom: 2px solid #327fac;") |>
    column_spec(1, extra_css = "border-left: 2px solid #327fac;") |>
    column_spec(dim(df)[2], extra_css = "border-right: 2px solid #327fac;")
}
```

## Introduction
This project explores what are the most valuable data science skills. To answer
this, the following methodology was followed:

*Data Collection* - import data from Bureau of Labor Statistics, United Nations
Standard Products and Services Code, Projections Central, and O*Net (data sources
are listed below). 

*Data Normalization* - clean and normalize the data by (1) removing unnecessary 
strings in the skills data frame so that the values are uniform within the 
`class_code` column, (2) replacing and matching strings within the OES Data 
through regex patterns, (3) and pivot the OES dataset wider, and then longer,
so that every row in the dataset is an observation (4) develop an hourly wage 
table by selecting values from the `soc` column that start with 'hr'. Then, 
remove NAs from the wage dataset, and pivot the data longer, and remove 
characters so that it numeric values be analyzed (5) develop an annual wage
table by selecting values from the `soc` column that start with 'annual' and then
follow a similar cleaning process in the step four (6) develop a segment wage
table by selecting values from the `soc` column that start with 'annual' and then
follow a similar cleaning process in the step four. ****REVIEW THIS*****

*Export to Database* - [insert sentence about where data is stored]

*Data Analysis* - [insert sentence about the completed data analysis]

*Summary of Findings* - [insert sentence about summarizing findings]

### Research Questions
1. Which skills are considered the most important in the data science field?
2. What is the relationship between skill importance and job compensation? How does this vary across industries?
3. ***What is relationship between projected employment and the importance of job skills?***

## Data Sources

-   [Industry Profile for Data Scientists](https://www.bls.gov/oes/current/oes150000.htm) - 
    This data source provides detailed information on Data Scientists. This 
    project uses the Bureau of Labor statistics to identify occupational codes
    of data scientists and relevant information such as income.
-   [United Nations Standard Products and Services Code](https://www.undp.org/unspsc) - 
    This data source includes information about products and services and can be
    used to analyze company expenditures.
-   [Projections Central](https://projectionscentral.org/directdownloads) - This data
    source includes projections of industry and occupational employment
    by state and the US. This could be used to explore the projected
    outcome for certain occupations related to data science such as data
    scientists, analysts, data engineers, and data architects.
-   [O\*Net Database](https://www.onetcenter.org/database.html#all-files) â€“ The
    O\*NET database outlines various information that describe work and
    worker characteristics, including skill requirements for many
    occupations. This data source may be used to explore skillsets,
    applications, and programming languages used in occupations related
    to data science.


## Read Data
```{r load-data}
# load the O*NET Skills Dataset, data dictionary available here: 
# https://www.onetcenter.org/dictionary/25.1/excel/knowledge.html
# skills <- read.csv("https://raw.githubusercontent.com/AlinaVikhnevich/data_607/refs/heads/main/Project%203/Data/Skills.csv")

# https://www.onetcenter.org/dictionary/29.2/excel/technology_skills.html
tech_skills = read.csv("https://raw.githubusercontent.com/AlinaVikhnevich/data_607/refs/heads/tidy_normalize_data_oa/Project%203/Data/Technology%20Skills.csv")

ep_project_skills = read.csv("https://raw.githubusercontent.com/AlinaVikhnevich/data_607/refs/heads/tidy_normalize_data_oa/Project%203/Data/public-skills-data.csv", skip = 1)

soc_industry_project = read.csv("https://raw.githubusercontent.com/AlinaVikhnevich/data_607/refs/heads/tidy_normalize_data_oa/Project%203/Data/National_Employment_Matrix_for_2023_and_projected_2033.csv")


# load the OES Data option to create custom table, then input data science and returned all related industries 
oes <- read.csv("https://raw.githubusercontent.com/AlinaVikhnevich/data_607/refs/heads/main/Project%203/Data/OES_Report_Industries_with_DS_Employment.csv",
                  skip = 6, nrows = 284,
                  header = FALSE)

# load the O*NET-SOC and SOC structure - reference file for occupational codes in:
# https://www.onetcenter.org/taxonomy/2019/structure.html
# soc_ref <- read.csv("https://raw.githubusercontent.com/AlinaVikhnevich/data_607/refs/heads/tidy_normalize_data_oa/Project%203/Data/SOC_Structure.csv", skip = 3)

# load the projected long-term employment from Projections Central (https://projectionscentral.org/directdownloads) **this is only if we decide to include projections
proj <- fromJSON("https://public.projectionscentral.org/Projections/LongTermRestJson")
proj <- as.data.frame(proj)
```

```{r ds-def}
# define data science occupation (SOC) code
data_science_soc = '15-2051'
```


### Tech Skills Data
```{r col-names-tech-skills}
# updated column names
colnames(tech_skills) = c(
  'onet_soc', 'onet_soc_title', 'tech_skill_example',
  'commodity_code', 'commodity_title', 'hot_technology',
  'in_demand')

# rendering table 
knit_table(head(tech_skills), 'View Raw Tech Skills Data')
```

Normalized tech skills data 
```{r skills-tech-tbls}
# create tech skills data reference tables
commodity_ref <- 
  tech_skills |>
  select(commodity_code, commodity_title, tech_skill_example) |>
  distinct()
```

```{r rem-ref-tbl}
# remove reference table columns from main data frame
tech_skills_df = tech_skills |>
  select('onet_soc','commodity_code', 'hot_technology','in_demand')

# rendering table 
knit_table(head(tech_skills_df), 'View Tidy Tech Skills Data')
```


### Skills Data

```{r skills-clean}
# update col names
colnames(ep_project_skills) = c(
  'soc_title', 'soc', 'employment_2023', 'employment_2033', 'employment_change_num_2023_33',
  'employment_change_prct_2023_33', 'ep_skills_category_id', 'ep_skills_category', 'ep_skills_score',
  'onet_soc', 'onet_element_id', 'onet_element_name', 'onet_rating_value'
  )

# rendering table
knit_table(head(ep_project_skills), 'View Raw Skills Data') |> scroll_box(width = "100%", box_css = "border: 1px solid #FFFFFF;")
```


```{r skills-norm}
# normalizing skills data 
skills_element_ref <- 
  ep_project_skills |>
  select(onet_element_id, onet_element_name) |>
  distinct()

# creating skills data reference tables
skills_category_ref <- ep_project_skills |>
  select('ep_skills_category_id', 'ep_skills_category') |>
  distinct()
```

```{r soc-onet-link}
# create soc and onet soc link table
soc_onet_soc_lnk <- 
  ep_project_skills |>
  select(soc, onet_soc) |>
  distinct()
```

```{r rem-ref-ep-skills}
# remove reference and link table columns from the main skills table
ep_skills_df = ep_project_skills |>
  select('soc', 'ep_skills_category_id', 'ep_skills_score',
         'onet_element_id', 'onet_rating_value') |>
  distinct()

# rendering table 
knit_table(head(ep_skills_df), 'View Tidy Skills Data')
```

### Industry and Occupation Projection Data

```{r soc-industries}
# update column names
colnames(soc_industry_project) = c(
  'soc_type', 'industry_type', 'soc', 'soc_title', 'industry_code', 'industry_title',
  'employment_2023', 'prct_industry_2023', 'prct_soc_2023', 
  'employment_2033', 'prct_industry_2033', 'prct_soc_2033',
  'employment_change_num_2023_33', 'employment_change_prct_2023_33')

# rendering table 
knit_table(head(ep_skills_df), 'View Raw Industry and Occupation Projection Data')
```

```{r soc-ind-proj}
# make employment columns to be in thousands as stated in raw data file 
soc_industry_project = soc_industry_project |>
  mutate_at(c('employment_2023', 'employment_2033', 'employment_change_num_2023_33'), ~(. *1000))
```

```{r soc-ind-lnk}
# create soc and industry code link table
soc_industry_lnk <- 
  soc_industry_project |>
  select(soc, industry_code) |>
  distinct()
```

```{r ind-ref}
# create industry and soc code reference tables
industry_ref <- 
  soc_industry_project |>
  select(industry_code, industry_title, industry_type) |>
  distinct()

# removing dups
soc_ref <- 
  soc_industry_project |>
  select(soc_title, soc, soc_type) |>
  distinct()

# selecting cols to visualize 
soc_industry_project_df = soc_industry_project |>
  select('industry_code', 'soc', 'employment_2023',
         'prct_industry_2023', 'prct_soc_2023', 'employment_2033', 'prct_industry_2033',
         'prct_soc_2033', 'employment_change_num_2023_33', 'employment_change_prct_2023_33')

# rendering table 
knit_table(head(soc_industry_project_df), 'View Tidy Industry and Occupation Projection Data') |> scroll_box(width = "100%", box_css = "border: 1px solid #FFFFFF;")
```

### OES Data
This data provides an industry profile for data science meaning the 
data is specifically based on industries with employment in Data Scientists.
```{r col-names-oes}
# changing col names
colnames(oes) = c(
  'occupation', 'employment', 'employment_prct_relative_se',
  'hr_wage_mean', 'annual_wage_mean', 'wage_prct_relative_se','hr_wage_10_prcentile',
  'hr_wage_25_prcentile', 'hr_wage_median', 'hr_wage_75_prcentile',
  'hr_wage_90_prcentile', 'annual_wage_10_prcentile',
  'annual_wage_25_prcentile', 'annual_wage_median', 'annual_wage_75_prcentile',
  'annual_wage_90_prcentile')

# rendering table 
knit_table(head(ep_project_skills), 'View Raw Data Science OES Industry Data') |> scroll_box(width = "100%", box_css = "border: 1px solid #FFFFFF;")
```

- pivot data to transform all hourly and annual metric columns into three columns:
    + wage_stat_duration: states if the statistic for the row is hourly annual
    + wage_statistic: text explaining the metric calculation
    + wage_value: provides the wage metric value for the provided wage_stat_duration and wage_statistic
```{r oes}
oes_df = oes |>
  # extract soc and industry codes from occupation string column
  mutate(soc = str_match(occupation, "\\s*(\\d{2}-\\d{4})")[,2])
  # categorize soc codes by structure
  # mutate(
  #   taxonomy = ifelse(
  #     endsWith(soc, '-0000'),'major_group', ifelse(
  #       grepl("\\d{2}-(1|2|3|4|5|6|7|8|9)\\d00", soc), 'minor_group', ifelse(
  #         grepl("\\d{2}-\\d{2}(1|2|3|4|5|6|7|8|9)0", soc), 'broad_occupation', ifelse(
  #           grepl("\\d{2}-\\d{3}(1|2|3|4|5|6|7|8|9)", soc), 'detailed_occupation', NA)))))

oes_df = oes_df |>
  # extract naics or industry code from occupation str column
  mutate(sector_naics_1 = stri_pad_right(str_match(
    occupation, "^Sector(s)*\\s{1}(.*)\\s-.*")[,3], 6, 0)) |>
  mutate(sector_naics_2 = str_match(occupation, "\\s*(\\d{2}---\\d{2})")[,2]) |>
  mutate(sector_naics_3 = str_replace(str_match(
    occupation, "\\s*(\\d{2}-\\d{2,}[A-Z]\\d*)")[,2], '-', '')) |>
  mutate(sector_naics = ifelse(
    !is.na(sector_naics_1), sector_naics_1, ifelse(
      !is.na(sector_naics_2), sector_naics_2, ifelse(
        !is.na(sector_naics_3), sector_naics_3, NA)))) |>
  mutate(occupation = gsub( " \\(.*$", "", occupation))

long_oes_df <- oes_df |> 
  pivot_longer(
    cols = matches('^(hr_|annual_)'), 
    names_to = c('wage_stat_duration', "wage_statistic"), 
    values_to = "wage_value",
    names_pattern="(hr|annual)_wage_(.*)"
    ) |>
  mutate(wage_value = str_remove(wage_value, "^\\$"))  # Removes "$"


knit_table(head(long_oes_df), 'View Data Science Wage Statistics Data') |> scroll_box(width = "100%", box_css = "border: 1px solid #FFFFFF;")
```

```{r soc-oes}
# split data into soc and industry tables 
soc_oes_df = long_oes_df |>
  # drop values not following soc structure (NAISC codes or sectors)
  drop_na(soc) |>
  select(soc, employment, wage_stat_duration, wage_statistic, wage_value)

# remove unneeded columns
industry_oes_df = long_oes_df |>
  filter(is.na(soc)) |>
  mutate(industry_code = str_replace(sector_naics, '---', '-')) |>
  select(industry_code, employment, wage_stat_duration, wage_statistic, wage_value)

# rendering tables
knit_table(head(soc_oes_df), 'View Tidy SOC Data Science OES Industry Data')
knit_table(head(industry_oes_df), 'View Tidy Industry Data Science OES Industry Data')
```


temp data science soc dfs (could be used for analysis or deleted of not)
```{r ep-skills}
# ds_ep_skills = ep_skills_df |>
#   filter(substr(soc, 0, str_count(data_science_soc)) == data_science_soc)
# 
# ds_ep_skills = left_join(ds_ep_skills, skills_element_ref)
# left_join(ds_ep_skills, skills_category_ref)
```

```{r soc-ind-proj}
# soc_industry_project_df |>
#   filter(substr(soc, 0, str_count(data_science_soc)) == data_science_soc)
```

temp data science tech skills df
```{r ds-tech-skills}
# ds_tech_skills = tech_skills_df |>
#   filter(substr(onet_soc, 0, str_count(data_science_soc)) == data_science_soc)
# 
# left_join(ds_tech_skills, commodity_ref)
```

### Tidy Data
**[Insert information about what the code about does, answer how does this makes
the data tidy. Feedback from Professor Kowalchuk: make sure you highlight the structure 
(tidy) of the analysis dataframe in the project submission.]**

## Industry tidy normalized tables



### Data Processing and Normalization
**[Insert information here about the data processing and normalization process.
Explain what makes the data tidy, and how it will be used to answer our 
research questions.]**

### MySQL Database
**[insert 1-2 sentences here about where data is stored, how the MySQL database 
is structured, and how it was configured.]**

#### Database Connection

```{r establish-connection}
# this is where we should establish the connection to the database

```

### Data Cleaning

```{r data-analysis}
# this is where we should analyze the data so that it can be visualized
```

### Data Analysis and Visualization

#### Question 1: Which skills are considered the most important in the data science field?

#### Question 2: What is the relationship between skill importance and job compensation? How does this vary across industries?

#### Question 3: What is relationship between projected employment and the importance of job skills?

### Summary of Findings
[insert conclusion paragraph here].
